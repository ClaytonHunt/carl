# Comprehensive Unit Test Suite - Story Intent
# Created: 2025-07-31T14:00:37-04:00
# Parent Feature: comprehensive-testing-strategy
# Scope: User Story

metadata:
  id: "comprehensive_unit_test_suite"
  scope_level: "story"
  created_date: "2025-07-31T14:00:37-04:00"
  created_by: "carl_plan_command"
  parent_feature: "comprehensive_testing_strategy"
  priority: "high"
  complexity: "high"
  estimated_effort: "1 week"

story_definition:
  name: "Comprehensive Unit Test Suite"
  description: "Complete unit test coverage with mocking for safe refactoring after migration"
  
  user_story: |
    As a CARL maintainer planning to refactor and improve the codebase,
    I want comprehensive unit tests covering all scenarios and edge cases,
    So that I can confidently make changes without introducing regressions.
  
  problem_statement: |
    After migration validation, need full unit test coverage to enable safe refactoring.
    Must test all functions, modules, edge cases, and error conditions with proper mocking
    to ensure any future changes don't break existing functionality.

acceptance_criteria:
  primary_scenarios:
    - scenario: "Helper function unit tests"
      given: "All helper functions ported to Node.js"
      when: "Running unit tests with mocked dependencies"
      then: "95% code coverage achieved with all scenarios tested"
      
    - scenario: "Hook script unit tests"
      given: "Hook scripts with dependency injection"
      when: "Testing with various input scenarios"
      then: "All code paths and error conditions covered"
      
    - scenario: "Edge case coverage"
      given: "Real-world scenarios and error conditions"
      when: "Running comprehensive test suite"
      then: "All edge cases handled correctly with appropriate responses"

technical_acceptance_criteria:
  test_coverage_requirements:
    - "95% code coverage across all modules"
    - "100% coverage of critical paths"
    - "All error conditions tested"
    - "Edge cases with proper assertions"
    
  testing_standards:
    - "Each function tested in isolation"
    - "Dependencies properly mocked"
    - "Realistic test data and fixtures"
    - "Clear test descriptions and documentation"

gherkin_scenarios:
  helper_function_testing: |
    Feature: Helper Function Unit Tests
      Background:
        Given helper modules are implemented
        And mocking infrastructure is set up

      Scenario: carl_get_active_context with valid file
        Given mock file system with active-work.carl
        And file contains valid CARL context
        When calling carl_get_active_context()
        Then should return formatted context string
        And should include project information
        And should match expected format

      Scenario: carl_get_active_context with missing file
        Given mock file system without active-work.carl
        When calling carl_get_active_context()
        Then should return empty string
        And should not throw error
        And should log debug message

      Scenario: carl_get_active_context with corrupted file
        Given mock file system with invalid YAML
        When calling carl_get_active_context()
        Then should return error message
        And should log error details
        And should not crash system

      Scenario: carl_get_setting with hierarchical config
        Given multiple config files:
          | level  | file           | audio_enabled |
          | global | global.conf    | true         |
          | team   | team.conf      | false        |
          | user   | user.conf      | true         |
        When calling carl_get_setting('audio_enabled')
        Then should return 'true' (user overrides)
        And should check files in correct order
        And should cache result appropriately

      Scenario: carl_play_audio with different platforms
        Given platform is detected as "<platform>"
        And audio is enabled
        When calling carl_play_audio('start', 'message')
        Then should use command "<audio_command>"
        And should handle missing audio files
        And should respect audio settings

        Examples:
          | platform | audio_command |
          | darwin   | afplay        |
          | linux    | aplay         |
          | win32    | start         |

  hook_script_testing: |
    Feature: Hook Script Unit Tests
      Background:
        Given hook scripts with dependency injection
        And comprehensive mocking setup

      Scenario: Session start with fresh installation
        Given no existing CARL files
        And mock helpers returning empty data
        When processing SessionStart event
        Then should display initialization message
        And should not create session files
        And should suggest setup commands
        And should play appropriate audio

      Scenario: Session start with existing project
        Given CARL project with:
          | vision.carl     | contains project info |
          | 5 intent files  | various work items    |
          | 3 state files   | completion tracking   |
        When processing SessionStart event
        Then should display vision first 10 lines
        And should show correct file counts
        And should create session with proper format
        And should update current session link

      Scenario: User prompt with CARL command detection
        Given prompt contains "/carl:task implement feature"
        And mock context returns active work data
        When processing through user-prompt hook
        Then should inject CARL context section
        And should add strategic context
        And should preserve original prompt
        And should format output correctly

      Scenario: Tool call with significant code changes
        Given Edit tool PostToolUse event
        And tool output indicates 25 lines changed
        When processing through tool-call hook
        Then should log milestone "significant_code_change"
        And should play progress audio
        And should update session metrics
        And should trigger celebration check

  error_handling_testing: |
    Feature: Error Handling and Edge Cases
      Background:
        Given error simulation capabilities
        And proper error handling implemented

      Scenario: File system errors
        Given file system operations that fail
        When attempting to read/write CARL files
        Then should handle errors gracefully
        And should provide meaningful error messages
        And should not crash Claude Code
        And should fall back to safe defaults

      Scenario: JSON parsing errors
        Given malformed JSON input from Claude Code
        When hook processes the input
        Then should detect parsing failure
        And should log error details
        And should return safe fallback
        And should not throw unhandled exceptions

      Scenario: Audio system failures
        Given audio commands that fail
        When attempting to play sounds
        Then should continue without audio
        And should log audio failure
        And should not block other operations

      Scenario: Unicode and special characters
        Given prompts with unicode: "implement 日本語 feature"
        And file paths with spaces: "/path with spaces/file.carl"
        When processing through hooks
        Then should preserve unicode correctly
        And should handle paths safely
        And should escape shell commands properly

      Scenario: Concurrent operations
        Given multiple simultaneous hook executions
        When modifying shared state files
        Then should handle concurrent access safely
        And should prevent file corruption
        And should maintain data consistency

implementation_approach:
  test_architecture: |
    /tests
      /unit
        /helpers
          carl-helpers.test.js      # Context, settings, utilities
          carl-session.test.js      # Session management
          carl-audio.test.js        # Audio playback
        /hooks
          session-start.test.js     # Session start behavior
          session-end.test.js       # Session end and summary
          user-prompt-submit.test.js # Context injection
          tool-call.test.js         # Activity tracking
      /fixtures
        projects/                   # Sample CARL projects
        events/                     # Claude Code event samples
        outputs/                    # Expected outputs
      /mocks
        fs-mock.js                 # File system operations
        child-process-mock.js      # External commands
        date-mock.js               # Consistent timestamps
      /utils
        test-helpers.js            # Common test utilities
        project-builder.js         # Test project creation

  mocking_strategy: |
    // Mock file system for isolated testing
    jest.mock('fs-extra', () => ({
      readFile: jest.fn(),
      writeFile: jest.fn(),
      ensureDir: jest.fn(),
      pathExists: jest.fn()
    }));
    
    // Mock child processes for audio and git
    jest.mock('child_process', () => ({
      exec: jest.fn(),
      spawn: jest.fn()
    }));
    
    // Mock date for consistent timestamps
    jest.mock('date-fns', () => ({
      format: jest.fn(() => '2025-07-31T14:00:37-04:00')
    }));

  test_examples: |
    describe('carl-helpers', () => {
      let mockFs;
      let helpers;
      
      beforeEach(() => {
        mockFs = require('fs-extra');
        mockFs.readFile.mockClear();
        mockFs.writeFile.mockClear();
        
        helpers = require('../lib/carl-helpers');
      });
      
      describe('getActiveContext', () => {
        it('should return formatted context from active work file', async () => {
          const mockContext = {
            active_intent: { id: 'features/user-auth' },
            work_queue: { in_progress: [] }
          };
          
          mockFs.readFile.mockResolvedValue(JSON.stringify(mockContext));
          
          const result = await helpers.getActiveContext();
          
          expect(result).toContain('features/user-auth');
          expect(mockFs.readFile).toHaveBeenCalledWith(
            expect.stringContaining('active-work.carl')
          );
        });
        
        it('should handle file not found gracefully', async () => {
          mockFs.readFile.mockRejectedValue(new Error('ENOENT'));
          
          const result = await helpers.getActiveContext();
          
          expect(result).toBe('');
          expect(console.error).not.toHaveBeenCalled();
        });
      });
    });

test_data_management:
  fixture_structure:
    - "Minimal fixtures for focused testing"
    - "Complex fixtures for integration scenarios"
    - "Error fixtures for edge case testing"
    - "Performance fixtures for load testing"
    
  snapshot_testing:
    - "Complex output format validation"
    - "Generated file content verification"
    - "Context injection format checking"
    - "Error message consistency"

continuous_testing:
  automated_execution:
    - "Run tests on every code change"
    - "Pre-commit hook validation"
    - "CI/CD integration"
    - "Coverage threshold enforcement"
    
  test_maintenance:
    - "Update tests with requirement changes"
    - "Add tests for reported bugs"
    - "Refactor tests to reduce duplication"
    - "Monitor and improve test performance"

success_definition:
  coverage_targets:
    - "95% overall code coverage"
    - "100% critical path coverage"
    - "All error conditions tested"
    - "Edge cases properly handled"
    
  quality_gates:
    - "All tests pass consistently"
    - "Test suite runs in under 30 seconds"
    - "No flaky or intermittent tests"
    - "Clear test failure messages"
    
  refactoring_readiness:
    - "Confident in making changes"
    - "Immediate feedback on regressions"
    - "Comprehensive scenario coverage"
    - "Documentation through tests"

dependencies:
  requires:
    - "Node.js hook scripts implemented"
    - "Helper modules with dependency injection"
    - "Jest testing framework configured"
    - "Mocking infrastructure set up"

next_actions:
  immediate: "Set up Jest configuration and mock infrastructure"
  implementation: "Write unit tests for helper functions first"
  expansion: "Add hook script tests and edge case coverage"
  maintenance: "Integrate with CI/CD and establish quality gates"